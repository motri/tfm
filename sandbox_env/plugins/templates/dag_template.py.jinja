"""
Auto-generated DAG based on UI input.
"""

from airflow import DAG
from airflow.operators.bash import BashOperator
from airflow.providers.ssh.operators.ssh import SSHOperator
from datetime import datetime
from operators.slurm_operator import SubmitAndMonitorSlurmJobOperator

default_args = {
    'owner': '{{ owner }}',
    'start_date': datetime({{ start_year }}, {{ start_month }}, {{ start_day }}),
    'retries': 0
}

with DAG(
    dag_id="{{ dag_id }}",
    default_args=default_args,
    schedule_interval="{{ schedule_interval }}",
    tags=["VICOMTECH"],
    catchup=False
) as dag:

{% if download_repo %}
    # Task 1: Clone the repository from GitLab
    download_repo = BashOperator(
        task_id="download_repo",
        bash_command="git clone {{ gitlab_repo_url }} /tmp/{{ repo_dir }}"
    )

    # Task 2: Copy the executable script to the HPC cluster using SSH (or scp)
    copy_to_hpc = SSHOperator(
        task_id="copy_script",
        ssh_conn_id="hpc_ssh",
        command="scp /tmp/{{ repo_dir }}/{{ script_file }} user@hpc:/destination/path/{{ script_file }}"
    )
{% endif %}

    # Task 3: Submit the SLURM job using the custom operator.
    # The 'script_path' here is actually our sbatch arguments.
    submit_slurm = SubmitAndMonitorSlurmJobOperator(
        task_id='submit_slurm',
        ssh_conn_id='hpc_ssh',
        sbatch_arguments="{{ batch_arguments }}"
    )

{% if download_repo %}
    # Define the task dependencies if a repo download is needed.
    download_repo >> copy_to_hpc >> submit_slurm
{% else %}
    # If no repo download is needed, just submit the job directly.
    submit_slurm
{% endif %}
