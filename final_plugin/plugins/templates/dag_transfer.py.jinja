from airflow import DAG
from airflow.operators.bash import BashOperator
from airflow.operators.trigger_dagrun import TriggerDagRunOperator
from datetime import datetime

default_args = {
    'owner': '{{ owner }}',
    'start_date': datetime({{ start_date_year }}, {{ start_date_month }}, {{ start_date_day }}),
}

with DAG(
    dag_id='{{ dag_id }}',
    default_args=default_args,
    schedule_interval=None,
    catchup=False,
    tags=[{{ workflow_tags|tojson }}]
) as dag:
    transfer = BashOperator(
        task_id='transfer_to_gpfs',
        bash_command=(
            'aws s3 sync '
            '{% if only_latest %}--exact-timestamps --delete {% endif %}'
            '{{ source_bucket }} {{ dest_path_gpfs }} '
            '{% if batch_size %}--max-concurrent-requests {{ parallel_transfers }}{% endif %}'
        )
    )

    {% if next_dag_id %}
    trigger = TriggerDagRunOperator(
        task_id='trigger_{{ next_dag_id }}',
        trigger_dag_id='{{ next_dag_id }}'
    )
    transfer >> trigger
    {% endif %}